# -*- coding: utf-8 -*-
"""Classifiers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQZ3qpKrAhLJminj7PYw15v8Fch2fA4a
"""

!pip install -r requirements.txt

import tensorflow.keras as keras
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import GRU, LSTM, Bidirectional, TimeDistributed
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import concatenate
import time
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import backend as K
from bert import tokenization
from utils import InputExample, convert_examples_to_features
from sklearn.metrics import *
import pickle

BERT_MODEL_PATH = "https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1"

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(curve = 'PR',name='auc'),
]

def load_embeddings_index():
    embeddings_index = dict()
    with open('embeddings/glove.6B.100d.txt', 'r') as glove_in:
        for line in glove_in.readlines():
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

class BERT(tf.keras.layers.Layer):
    """
    Extending the code from:
    https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2
    The layers to fine tuned are selected by name.
    """
    def __init__(
            self,
            n_fine_tune_top_layers=10,
            trainable=True,
            pooling="first",
            output_size=768,
            **kwargs,
    ):
        self.n_fine_tune_layers = n_fine_tune_top_layers
        self.trainable = trainable
        self.output_size = output_size
        self.pooling = pooling
        self.bert_path = BERT_MODEL_PATH
        if self.pooling not in ["first", "mean"]:
            raise NameError(f"Undefined pooling type (must be either first or mean, but is {self.pooling}")
        super(BERT, self).__init__(**kwargs)

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'n_fine_tune_top_layers': self.n_fine_tune_top_layers,
            'trainable': self.trainable,
            'bert': self.bert,
            'pooling': self.pooling,
        })
        return config

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path, trainable=self.trainable, name=f"{self.name}_module"
        )

        # Remove unused layers
        trainable_vars = self.bert.variables
        if self.pooling == "first":
            trainable_vars = [var for var in trainable_vars if not "/cls/" in var.name]
            trainable_layers = ["pooler/dense"]

        elif self.pooling == "mean":
            trainable_vars = [
                var
                for var in trainable_vars
                if not "/cls/" in var.name and not "/pooler/" in var.name
            ]
            trainable_layers = []
        else:
            raise NameError(
                f"Undefined pooling type (must be either first or mean, but is {self.pooling}"
            )

        # Select how many layers to fine tune
        for i in range(self.n_fine_tune_layers):
            trainable_layers.append(f"encoder/layer_{str(11 - i)}")

        # Update trainable vars to contain only the specified layers
        trainable_vars = [
            var
            for var in trainable_vars
            if any([l in var.name for l in trainable_layers])
        ]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BERT, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype="int32") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        if self.pooling == "first":
            pooled = self.bert(inputs=bert_inputs, signature="tokens", as_dict=True)[
                "pooled_output"
            ]
        elif self.pooling == "mean":
            result = self.bert(inputs=bert_inputs, signature="tokens", as_dict=True)[
                "sequence_output"
            ]

            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)
            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (
                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)
            input_mask = tf.cast(input_mask, tf.float32)
            pooled = masked_reduce_mean(result, input_mask)
        else:
            raise NameError(f"Undefined pooling type (must be either first or mean, but is {self.pooling}")

        return pooled

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_size)

#BERT MLP FOR REGRESSION
class BERT_MLP():

    def __init__(self,
                 trainable_layers=3,
                 max_seq_length=128,
                 show_summary=False,
                 patience=3,
                 seed=42,
                 epochs=10,
                 save_predictions=False,
                 batch_size=32,
                 DATA_COLUMN="text",
                 TARGET_COLUMN="target",
                 DATA2_COLUMN=None,
                 lr=2e-05,
                 session=None,
                 loss="binary_crossentropy"
                 ):
        self.session = session
        tf.compat.v1.set_random_seed(seed)
        np.random.seed(seed)
        self.name = f'{"OOC1" if not DATA2_COLUMN else "OOC2"}-b{batch_size}.e{epochs}.len{max_seq_length}.bert'
        self.tokenizer = self.create_tokenizer_from_hub_module()
        self.lr = lr
        self.batch_size = batch_size
        self.DATA_COLUMN=DATA_COLUMN
        self.DATA2_COLUMN=DATA2_COLUMN
        self.TARGET_COLUMN=TARGET_COLUMN
        self.trainable_layers = trainable_layers
        self.max_seq_length = max_seq_length
        self.show_summary = show_summary
        self.patience=patience
        self.save_predictions = save_predictions
        self.epochs = epochs
        self.loss = loss
        self.earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_auc',
                                                          patience=self.patience,
                                                          verbose=1,
                                                          restore_best_weights=True,
                                                          mode="max")

    def build(self, bias=0):
        in_id = tf.keras.layers.Input(shape=(self.max_seq_length,), name="input_ids")
        in_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), name="input_masks")
        in_segment = tf.keras.layers.Input(shape=(self.max_seq_length,), name="segment_ids")
        bert_inputs = [in_id, in_mask, in_segment]
        bert_output = BERT(n_fine_tune_top_layers=self.trainable_layers)(bert_inputs)
        dense = tf.keras.layers.Dense(128, activation='tanh')(bert_output)
        pred = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=tf.keras.initializers.Constant(bias))(dense)
        self.model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
        self.model.compile(loss=self.loss,
                      optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr),
                      metrics=METRICS)
        if self.show_summary:
            self.model.summary()

    def get_features(self, features):
        input_ids, input_masks, segment_ids, targets = [], [], [], []
        for f in features:
            input_ids.append(f.input_ids)
            input_masks.append(f.input_mask)
            segment_ids.append(f.segment_ids)
            targets.append(f.target_id)
        return (np.array(input_ids), np.array(input_masks), np.array(segment_ids), np.array(targets).reshape(-1, 1),)

    def to_bert_input(self, dataset_pd):
        x_input = dataset_pd.apply(lambda x: InputExample(guid=None,
                                                          text_a=x[self.DATA_COLUMN],
                                                          text_b=x[self.DATA2_COLUMN] if self.DATA2_COLUMN else None,
                                                          target=x[self.TARGET_COLUMN]), axis=1)
        x_features = convert_examples_to_features(x_input,
                                                  self.max_seq_length,
                                                  self.tokenizer)
        x_input_ids, x_input_masks, x_segment_ids, x_targets = self.get_features(x_features)
        return (x_input_ids, x_input_masks, x_segment_ids), x_targets

    def fit(self, train, dev, bert_weights=None, class_weights={0: 1, 1: 1}, pretrained_embeddings=None):
        train_input, train_targets = self.to_bert_input(train)
        dev_input, dev_targets = self.to_bert_input(dev)
        # pos = sum(train_labels)
        # neg = len(train_labels)-pos
        # bias = np.log(pos/neg)
        # print ("BIAS:", bias)
        self.build()
        if bert_weights is not None:
            self.model.load_weights(bert_weights)
        self.initialise_vars() # instantiation needs to be right before fitting
        self.model.fit(train_input,
                       train_targets,
                       validation_data=(dev_input, dev_targets),
                       epochs=self.epochs,
                       callbacks=[self.earlystop],
                       batch_size=self.batch_size,
                       class_weight=None 
                       )

    def predict(self, val_pd):
        #with self.session.as_default():
        val_input, val_targets = self.to_bert_input(val_pd)
        predictions = self.model.predict(val_input)
        print('Stopped epoch: ', self.earlystop.stopped_epoch)
        if self.save_predictions:
            self.save_evaluation_set(val_targets, predictions)
        return predictions

    def save_evaluation_set(self, gold, predictions):
        logtime = time.strftime('%Y%m%d-%H%M%S')
        pd.DataFrame({"gold":gold, "pred":predictions}).to_csv(f"{self.name}.{logtime}.evaluation.csv")

    def create_tokenizer_from_hub_module(self):
        bert_module = hub.Module(BERT_MODEL_PATH)
        tokenization_info = bert_module(signature="tokenization_info", as_dict=True)
        vocab_file, do_lower_case = self.session.run([tokenization_info["vocab_file"],tokenization_info["do_lower_case"]])
        return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

    def initialise_vars(self):
        self.session.run(tf.local_variables_initializer())
        self.session.run(tf.global_variables_initializer())
        self.session.run(tf.tables_initializer())
        K.set_session(self.session)

    def save(self):
        self.model.save(f"{self.name}.weights.h5")

    def model_show(self):
        print(self.model.summary())

"""# CA BERT&BERT (concatenate  the CLS tokens of parent and target)"""

class CA_BERT_MLP():

    def __init__(self,
                 trainable_layers=3,
                 max_seq_length=128,
                 show_summary=False,
                 patience=3,
                 seed=42,
                 epochs=10,
                 save_predictions=False,
                 batch_size=32,
                 DATA_COLUMN="text",
                 TARGET_COLUMN="target",
                 DATA2_COLUMN=None,
                 lr=2e-05,
                 session=None,
                 loss="binary_crossentropy"
                 ):
        self.session = session
        tf.compat.v1.set_random_seed(seed)
        np.random.seed(seed)
        self.name = f'{"OOC1" if not DATA2_COLUMN else "OOC2"}-b{batch_size}.e{epochs}.len{max_seq_length}.bert'
        self.tokenizer = self.create_tokenizer_from_hub_module()
        self.lr = lr
        self.batch_size = batch_size
        self.DATA_COLUMN=DATA_COLUMN
        self.DATA2_COLUMN=DATA2_COLUMN
        self.TARGET_COLUMN=TARGET_COLUMN
        self.trainable_layers = trainable_layers
        self.max_seq_length = max_seq_length
        self.show_summary = show_summary
        self.patience=patience
        self.save_predictions = save_predictions
        self.epochs = epochs
        self.loss = loss
        self.earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_auc',
                                                          patience=self.patience,
                                                          verbose=1,
                                                          restore_best_weights=True,
                                                          mode="max")

    def build(self, bias=0):
        #take the 'CLS' token of the parent 

        parent_in_id = tf.keras.layers.Input(shape=(self.max_seq_length,), name="parent_input_ids")
        parent_in_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), name="parent_input_masks")
        parent_in_segment = tf.keras.layers.Input(shape=(self.max_seq_length,), name="parent_segment_ids")
        bert_parent_inputs = [parent_in_id, parent_in_mask, parent_in_segment]
        bert_parent_output = BERT(n_fine_tune_top_layers=self.trainable_layers)(bert_parent_inputs)


        #take the 'CLS' token of the target
        in_id = tf.keras.layers.Input(shape=(self.max_seq_length,), name="input_ids")
        in_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), name="input_masks")
        in_segment = tf.keras.layers.Input(shape=(self.max_seq_length,), name="segment_ids")
        bert_inputs = [in_id, in_mask, in_segment]
        bert_output = BERT(n_fine_tune_top_layers=self.trainable_layers)(bert_inputs)

        #Concatenate the 'CLS' tokens
        x = concatenate([bert_parent_output, bert_output])
        print(x.shape, "sizee")

        dense = tf.keras.layers.Dense(128, activation='tanh')(x)
        pred = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=tf.keras.initializers.Constant(bias))(dense)
        self.model = tf.keras.models.Model(inputs=bert_parent_inputs + bert_inputs, outputs=pred)
        self.model.compile(loss=self.loss,
                      optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr),
                      metrics=METRICS)
        if self.show_summary:
            self.model.summary()

    def get_features(self, features):
        input_ids, input_masks, segment_ids, targets = [], [], [], []
        for f in features:
            input_ids.append(f.input_ids)
            input_masks.append(f.input_mask)
            segment_ids.append(f.segment_ids)
            targets.append(f.target_id)
        return (np.array(input_ids), np.array(input_masks), np.array(segment_ids), np.array(targets).reshape(-1, 1),)

    def to_bert_input(self, dataset_pd, text_to_encode):

        x_input = dataset_pd.apply(lambda x: InputExample(guid=None,
                                                          text_a=x[text_to_encode],
                                                          text_b=x[self.DATA2_COLUMN] if self.DATA2_COLUMN else None,
                                                          target=x[self.TARGET_COLUMN]), axis=1)

        x_features = convert_examples_to_features(x_input,
                                                  self.max_seq_length,
                                                  self.tokenizer)
        x_input_ids, x_input_masks, x_segment_ids, x_targets = self.get_features(x_features)
        return (x_input_ids, x_input_masks, x_segment_ids), x_targets

    def fit(self, train, dev, bert_weights=None, class_weights={0: 1, 1: 1}, pretrained_embeddings=None):
        #encode parent (ignore second return arg)
        parent_input, _ = self.to_bert_input(train, text_to_encode="parent")
        parent_dev, _ = self.to_bert_input(dev, text_to_encode="parent")

        #encode target
        train_input, train_targets = self.to_bert_input(train, text_to_encode="text")
        dev_input, dev_targets = self.to_bert_input(dev, text_to_encode="text")

        self.build()
        if bert_weights is not None:
            self.model.load_weights(bert_weights)
        self.initialise_vars() # instantiation needs to be right before fitting
        self.model.fit(list(parent_input) + list(train_input),
                       train_targets,
                       validation_data=(list(parent_dev) + list(dev_input) , dev_targets),
                       epochs=self.epochs,
                       callbacks=[self.earlystop],
                       batch_size=self.batch_size,
                       class_weight=None 
                       )

    def predict(self, val_pd):
        #with self.session.as_default():

        #encode parent 
        parent_val_input, _ = self.to_bert_input(val_pd, text_to_encode="parent")

        #encode target
        val_input, val_targets = self.to_bert_input(val_pd, text_to_encode="text")

        predictions = self.model.predict(parent_val_input + val_input)
        print('Stopped epoch: ', self.earlystop.stopped_epoch)
        if self.save_predictions:
            self.save_evaluation_set(val_targets, predictions)
        return predictions

    def save_evaluation_set(self, gold, predictions):
        logtime = time.strftime('%Y%m%d-%H%M%S')
        pd.DataFrame({"gold":gold, "pred":predictions}).to_csv(f"{self.name}.{logtime}.evaluation.csv")

    def create_tokenizer_from_hub_module(self):
        bert_module = hub.Module(BERT_MODEL_PATH)
        tokenization_info = bert_module(signature="tokenization_info", as_dict=True)
        vocab_file, do_lower_case = self.session.run([tokenization_info["vocab_file"],tokenization_info["do_lower_case"]])
        return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)

    def initialise_vars(self):
        self.session.run(tf.local_variables_initializer())
        self.session.run(tf.global_variables_initializer())
        self.session.run(tf.tables_initializer())
        K.set_session(self.session)

    def save(self):
        self.model.save(f"{self.name}.weights.h5")

    def model_show(self):
        print(self.model.summary())